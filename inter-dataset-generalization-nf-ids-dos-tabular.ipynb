{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9be65bd",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.041982,
     "end_time": "2023-03-29T11:31:03.404912",
     "exception": false,
     "start_time": "2023-03-29T11:31:03.362930",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fastAI\n",
      "  Downloading fastai-2.7.12-py3-none-any.whl (233 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.1/233.1 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from fastAI) (23.0)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from fastAI) (6.0)\n",
      "Collecting torchvision>=0.8.2\n",
      "  Downloading torchvision-0.15.1-cp310-cp310-manylinux1_x86_64.whl (6.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from fastAI) (3.7.1)\n",
      "Collecting spacy<4\n",
      "  Downloading spacy-3.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from fastAI) (1.10.1)\n",
      "Collecting fastdownload<2,>=0.0.5\n",
      "  Downloading fastdownload-0.0.7-py3-none-any.whl (12 kB)\n",
      "Collecting fastcore<1.6,>=1.5.29\n",
      "  Downloading fastcore-1.5.29-py3-none-any.whl (67 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.6/67.6 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from fastAI) (2.28.2)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from fastAI) (1.2.2)\n",
      "Collecting fastprogress>=0.2.4\n",
      "  Downloading fastprogress-1.0.3-py3-none-any.whl (12 kB)\n",
      "Collecting torch<2.1,>=1.7\n",
      "  Downloading torch-2.0.0-cp310-cp310-manylinux1_x86_64.whl (619.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from fastAI) (1.5.3)\n",
      "Requirement already satisfied: pillow>6.0.0 in /opt/conda/lib/python3.10/site-packages (from fastAI) (9.4.0)\n",
      "Requirement already satisfied: pip in /opt/conda/lib/python3.10/site-packages (from fastAI) (23.0.1)\n",
      "Collecting cymem<2.1.0,>=2.0.2\n",
      "  Downloading cymem-2.0.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34 kB)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from spacy<4->fastAI) (67.3.2)\n",
      "Collecting wasabi<1.2.0,>=0.9.1\n",
      "  Downloading wasabi-1.1.1-py3-none-any.whl (27 kB)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4\n",
      "  Downloading pydantic-1.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m49.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hCollecting spacy-legacy<3.1.0,>=3.0.11\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Collecting langcodes<4.0.0,>=3.2.0\n",
      "  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.6/181.6 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pathy>=0.10.0\n",
      "  Downloading pathy-0.10.1-py3-none-any.whl (48 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.9/48.9 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.15.0 in /opt/conda/lib/python3.10/site-packages (from spacy<4->fastAI) (1.23.5)\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Downloading preshed-3.0.8-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.7/124.7 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from spacy<4->fastAI) (3.1.2)\n",
      "Collecting srsly<3.0.0,>=2.4.3\n",
      "  Downloading srsly-2.4.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (492 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m492.2/492.2 kB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting smart-open<7.0.0,>=5.2.1\n",
      "  Downloading smart_open-6.3.0-py3-none-any.whl (56 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting thinc<8.2.0,>=8.1.8\n",
      "  Downloading thinc-8.1.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (910 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m911.0/911.0 kB\u001b[0m \u001b[31m41.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting typer<0.8.0,>=0.3.0\n",
      "  Downloading typer-0.7.0-py3-none-any.whl (38 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Downloading murmurhash-1.0.9-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6\n",
      "  Downloading catalogue-2.0.8-py3-none-any.whl (17 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
      "  Downloading spacy_loggers-1.0.4-py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.10/site-packages (from spacy<4->fastAI) (4.64.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->fastAI) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->fastAI) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->fastAI) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->fastAI) (1.26.14)\n",
      "Collecting filelock\n",
      "  Downloading filelock-3.10.7-py3-none-any.whl (10 kB)\n",
      "Collecting nvidia-nvtx-cu11==11.7.91\n",
      "  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.6/98.6 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusparse-cu11==11.7.4.91\n",
      "  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cublas-cu11==11.10.3.66\n",
      "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cudnn-cu11==8.5.0.96\n",
      "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting triton==2.0.0\n",
      "  Downloading triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-curand-cu11==10.2.10.91\n",
      "  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nccl-cu11==2.14.3\n",
      "  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.1/177.1 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cufft-cu11==10.9.0.58\n",
      "  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.7.99\n",
      "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch<2.1,>=1.7->fastAI) (3.0)\n",
      "Collecting nvidia-cusolver-cu11==11.4.0.1\n",
      "  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.6/102.6 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-cupti-cu11==11.7.101\n",
      "  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m53.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu11==11.7.99\n",
      "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m35.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch<2.1,>=1.7->fastAI) (1.11.1)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch<2.1,>=1.7->fastAI) (4.4.0)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch<2.1,>=1.7->fastAI) (0.38.4)\n",
      "Collecting cmake\n",
      "  Downloading cmake-3.26.1-py2.py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (24.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.0/24.0 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting lit\n",
      "  Downloading lit-16.0.0.tar.gz (144 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m145.0/145.0 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->fastAI) (1.0.7)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->fastAI) (4.39.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->fastAI) (0.11.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib->fastAI) (2.8.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->fastAI) (1.4.4)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->fastAI) (3.0.9)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->fastAI) (2022.7.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->fastAI) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->fastAI) (3.1.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib->fastAI) (1.16.0)\n",
      "Collecting blis<0.8.0,>=0.7.8\n",
      "  Downloading blis-0.7.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hCollecting confection<1.0.0,>=0.0.1\n",
      "  Downloading confection-0.0.4-py3-none-any.whl (32 kB)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /opt/conda/lib/python3.10/site-packages (from typer<0.8.0,>=0.3.0->spacy<4->fastAI) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->spacy<4->fastAI) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch<2.1,>=1.7->fastAI) (1.3.0)\n",
      "Building wheels for collected packages: lit\n",
      "  Building wheel for lit (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for lit: filename=lit-16.0.0-py3-none-any.whl size=93582 sha256=158b7faf07ad55afe49e196a2fada538c18b3a8fb8ae8cd91d659f8b361119b4\n",
      "  Stored in directory: /home/rtalwaru/.cache/pip/wheels/a3/f5/8f/b11227e816563ac08fce423c6e617f05f5fe1a18d9bcfb2375\n",
      "Successfully built lit\n",
      "Installing collected packages: lit, cymem, cmake, wasabi, typer, spacy-loggers, spacy-legacy, smart-open, pydantic, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, murmurhash, langcodes, filelock, fastprogress, fastcore, catalogue, blis, srsly, preshed, pathy, nvidia-cusolver-cu11, nvidia-cudnn-cu11, fastdownload, confection, thinc, spacy, triton, torch, torchvision, fastAI\n",
      "Successfully installed blis-0.7.9 catalogue-2.0.8 cmake-3.26.1 confection-0.0.4 cymem-2.0.7 fastAI-2.7.12 fastcore-1.5.29 fastdownload-0.0.7 fastprogress-1.0.3 filelock-3.10.7 langcodes-3.3.0 lit-16.0.0 murmurhash-1.0.9 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 pathy-0.10.1 preshed-3.0.8 pydantic-1.10.7 smart-open-6.3.0 spacy-3.5.1 spacy-legacy-3.0.12 spacy-loggers-1.0.4 srsly-2.4.6 thinc-8.1.9 torch-2.0.0 torchvision-0.15.1 triton-2.0.0 typer-0.7.0 wasabi-1.1.1\n"
     ]
    }
   ],
   "source": [
    "! pip install fastAI\n",
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "104f97a7",
   "metadata": {
    "papermill": {
     "duration": 46.506484,
     "end_time": "2023-03-29T11:31:49.914860",
     "exception": false,
     "start_time": "2023-03-29T11:31:03.408376",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "datasets={}\n",
    "for dirname, _, filenames in os.walk('/project_ghent/raman/netflow_datasets'):\n",
    "    for filename in filenames:\n",
    "        if filename.endswith(\"parquet\"):\n",
    "            datasets[filename.replace(\".parquet\",\"\")]=pd.read_parquet(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170e97b8",
   "metadata": {
    "papermill": {
     "duration": 0.002701,
     "end_time": "2023-03-29T11:31:49.920955",
     "exception": false,
     "start_time": "2023-03-29T11:31:49.918254",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- DoS is available in all 4 datasets\n",
    "- DDoS is available in 3 out of 4 datasets\n",
    "- Backdoor in 2 out of 4 datasets\n",
    "- Reconnaissance in 2 ou of 4 datasets\n",
    "\n",
    "let's start with DoS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "399d1d88",
   "metadata": {
    "papermill": {
     "duration": 4.063389,
     "end_time": "2023-03-29T11:31:53.987216",
     "exception": false,
     "start_time": "2023-03-29T11:31:49.923827",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ransomware' 'Benign' 'xss' 'scanning' 'password' 'dos' 'ddos'\n",
      " 'injection' 'mitm' 'backdoor']\n",
      "['Benign' 'Exploits' 'Generic' 'Fuzzers' 'Backdoor' 'DoS' 'Reconnaissance'\n",
      " 'Shellcode' 'Worms' 'Analysis']\n",
      "['DoS' 'DDoS' 'Reconnaissance' 'Benign' 'Theft']\n",
      "['SSH-Bruteforce' 'Benign' 'DDoS attacks-LOIC-HTTP' 'DDOS attack-HOIC'\n",
      " 'DoS attacks-Slowloris' 'DoS attacks-Hulk' 'FTP-BruteForce'\n",
      " 'Infilteration' 'Bot' 'DoS attacks-GoldenEye' 'Brute Force -Web'\n",
      " 'DoS attacks-SlowHTTPTest' 'SQL Injection' 'DDOS attack-LOIC-UDP'\n",
      " 'Brute Force -XSS']\n"
     ]
    }
   ],
   "source": [
    "for k,v in datasets.items():\n",
    "    print(v.Attack.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb17eb0f",
   "metadata": {
    "papermill": {
     "duration": 41.064777,
     "end_time": "2023-03-29T11:32:35.055586",
     "exception": false,
     "start_time": "2023-03-29T11:31:53.990809",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2883/191127425.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dos.drop(index=ddos.index,inplace=True)\n",
      "/tmp/ipykernel_2883/191127425.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dos.Attack=\"DoS\"#uniform name\n",
      "/tmp/ipykernel_2883/191127425.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dos.drop(index=ddos.index,inplace=True)\n",
      "/tmp/ipykernel_2883/191127425.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dos.Attack=\"DoS\"#uniform name\n",
      "/tmp/ipykernel_2883/191127425.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dos.drop(index=ddos.index,inplace=True)\n",
      "/tmp/ipykernel_2883/191127425.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dos.Attack=\"DoS\"#uniform name\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ds,data \u001b[38;5;129;01min\u001b[39;00m datasets\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m----> 2\u001b[0m     ddos\u001b[38;5;241m=\u001b[39mdata[\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mAttack\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontains\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mddos\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mcase\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m]\n\u001b[1;32m      3\u001b[0m     dos\u001b[38;5;241m=\u001b[39mdata[data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAttack\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mcontains(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdos\u001b[39m\u001b[38;5;124m\"\u001b[39m,case\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)]\n\u001b[1;32m      4\u001b[0m     dos\u001b[38;5;241m.\u001b[39mdrop(index\u001b[38;5;241m=\u001b[39mddos\u001b[38;5;241m.\u001b[39mindex,inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/strings/accessor.py:129\u001b[0m, in \u001b[0;36mforbid_nonstring_types.<locals>._forbid_nonstring_types.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    124\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    125\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot use .str.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with values of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    126\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minferred dtype \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inferred_dtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    127\u001b[0m     )\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 129\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/strings/accessor.py:1260\u001b[0m, in \u001b[0;36mStringMethods.contains\u001b[0;34m(self, pat, case, flags, na, regex)\u001b[0m\n\u001b[1;32m   1252\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m regex \u001b[38;5;129;01mand\u001b[39;00m re\u001b[38;5;241m.\u001b[39mcompile(pat)\u001b[38;5;241m.\u001b[39mgroups:\n\u001b[1;32m   1253\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1254\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis pattern is interpreted as a regular expression, and has \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1255\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmatch groups. To actually get the groups, use str.extract.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1256\u001b[0m         \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[1;32m   1257\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m   1258\u001b[0m     )\n\u001b[0;32m-> 1260\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_str_contains\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcase\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mregex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1261\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrap_result(result, fill_value\u001b[38;5;241m=\u001b[39mna, returns_string\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/strings/object_array.py:131\u001b[0m, in \u001b[0;36mObjectStringArrayMixin._str_contains\u001b[0;34m(self, pat, case, flags, na, regex)\u001b[0m\n\u001b[1;32m    129\u001b[0m         upper_pat \u001b[38;5;241m=\u001b[39m pat\u001b[38;5;241m.\u001b[39mupper()\n\u001b[1;32m    130\u001b[0m         f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m x: upper_pat \u001b[38;5;129;01min\u001b[39;00m x\u001b[38;5;241m.\u001b[39mupper()\n\u001b[0;32m--> 131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_str_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbool\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/strings/object_array.py:68\u001b[0m, in \u001b[0;36mObjectStringArrayMixin._str_map\u001b[0;34m(self, f, na_value, dtype, convert)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([], dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m     67\u001b[0m arr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(\u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mobject\u001b[39m)\n\u001b[0;32m---> 68\u001b[0m mask \u001b[38;5;241m=\u001b[39m \u001b[43misna\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m map_convert \u001b[38;5;241m=\u001b[39m convert \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39mall(mask)\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/dtypes/missing.py:185\u001b[0m, in \u001b[0;36misna\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21misna\u001b[39m(obj: \u001b[38;5;28mobject\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m|\u001b[39m npt\u001b[38;5;241m.\u001b[39mNDArray[np\u001b[38;5;241m.\u001b[39mbool_] \u001b[38;5;241m|\u001b[39m NDFrame:\n\u001b[1;32m    109\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;124;03m    Detect missing values for an array-like object.\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;124;03m    Name: 1, dtype: bool\u001b[39;00m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_isna\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/dtypes/missing.py:214\u001b[0m, in \u001b[0;36m_isna\u001b[0;34m(obj, inf_as_na)\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, (np\u001b[38;5;241m.\u001b[39mndarray, ABCExtensionArray)):\n\u001b[0;32m--> 214\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_isna_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minf_as_na\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minf_as_na\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, ABCIndex):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# Try to use cached isna, which also short-circuits for integer dtypes\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m#  and avoids materializing RangeIndex._values\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_can_hold_na:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/dtypes/missing.py:296\u001b[0m, in \u001b[0;36m_isna_array\u001b[0;34m(values, inf_as_na)\u001b[0m\n\u001b[1;32m    294\u001b[0m         result \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39misna()  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_string_or_object_np_dtype(values\u001b[38;5;241m.\u001b[39mdtype):\n\u001b[0;32m--> 296\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_isna_string_dtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minf_as_na\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minf_as_na\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m needs_i8_conversion(dtype):\n\u001b[1;32m    298\u001b[0m     \u001b[38;5;66;03m# this is the NaT pattern\u001b[39;00m\n\u001b[1;32m    299\u001b[0m     result \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mi8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m==\u001b[39m iNaT\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/dtypes/missing.py:318\u001b[0m, in \u001b[0;36m_isna_string_dtype\u001b[0;34m(values, inf_as_na)\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m values\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 318\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mlibmissing\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misnaobj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minf_as_na\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minf_as_na\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m values\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m    320\u001b[0m         result \u001b[38;5;241m=\u001b[39m libmissing\u001b[38;5;241m.\u001b[39misnaobj2d(values, inf_as_na\u001b[38;5;241m=\u001b[39minf_as_na)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for ds,data in datasets.items():\n",
    "    ddos=data[data['Attack'].str.contains(\"ddos\",case=False)]\n",
    "    dos=data[data['Attack'].str.contains(\"dos\",case=False)]\n",
    "    dos.drop(index=ddos.index,inplace=True)\n",
    "    dos.Attack=\"DoS\"#uniform name\n",
    "    benign=data[data['Attack'] == 'Benign']\n",
    "    \n",
    "    size_dos=dos.shape[0]\n",
    "    size_benign= benign.shape[0]\n",
    "    \n",
    "    if size_dos>size_benign:\n",
    "        #downsample attack\n",
    "        dos= dos.sample(size_benign)\n",
    "    else:\n",
    "        #downsample benign\n",
    "        benign = benign.sample(size_dos)\n",
    "    \n",
    "    data = pd.concat(objs=[dos, benign])\n",
    "    \n",
    "    # random forest classifier works internally with float32 so values too big will give errors\n",
    "    data = data[data['SRC_TO_DST_SECOND_BYTES'] <= np.finfo(np.float32).max]\n",
    "    data = data[data['DST_TO_SRC_SECOND_BYTES'] <= np.finfo(np.float32).max]\n",
    "    \n",
    "    data=data.drop(columns=['L4_SRC_PORT', 'L4_DST_PORT']) # dropping metadata\n",
    "\n",
    "    datasets[ds]=data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf5c76b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for k,v in datasets.items():\n",
    "    print(v.Attack.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d68d6c",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b321ed",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "for k,v in datasets.items():\n",
    "    x=v[v.columns[:-2]]\n",
    "    y=v[v.columns[-2]]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(x, y,\n",
    "                                                    stratify=y, \n",
    "                                                    test_size=0.75)\n",
    "    scaler = StandardScaler()\n",
    "    X_train=scaler.fit_transform(X_train)\n",
    "    X_test=scaler.transform(X_test)\n",
    "    y_train.reset_index(drop=True,inplace=True)\n",
    "    y_test.reset_index(drop=True,inplace=True)\n",
    "    datasets[k]={\"X_train\":X_train, \"X_test\":X_test, \"y_train\":y_train, \"y_test\":y_test}\n",
    "    print(f\"{k}: {y_train.shape[0]} {y_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d71109f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### inter-dataset evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f1898e",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from fastai.tabular.all import * \n",
    "\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, precision_score, recall_score, f1_score, accuracy_score\n",
    "dataset_names = datasets.keys()\n",
    "prepared_result_rows = []\n",
    "for dataset_train in dataset_names:\n",
    "    for dataset_test in dataset_names:\n",
    "            accuracies = []\n",
    "            precisions = []\n",
    "            recalls = []\n",
    "            tr_df = pd.concat(objs=[pd.DataFrame(datasets[dataset_train][\"X_train\"]), datasets[dataset_train][\"y_train\"]], axis=1) \n",
    "            te_df = pd.concat(objs=[pd.DataFrame(datasets[dataset_test][\"X_test\"]), datasets[dataset_test][\"y_test\"]], axis=1)\n",
    "            \n",
    "            for i in range(1):\n",
    "                splits = TrainTestSplitter(test_size=0.2, stratify=tr_df['Label'])(range_of(tr_df))\n",
    "                fast_train = TabularPandas(\n",
    "                    df=tr_df, procs=[], cat_names=None, cont_names=tr_df.columns[:-1].values.tolist(), y_names='Label',\n",
    "                    y_block=CategoryBlock(), splits=splits, do_setup=True, reduce_memory=False)\n",
    "                \n",
    "                dls = fast_train.dataloaders(bs=128, device='cuda')\n",
    "                model = tabular_learner(layers=[20,10], dls=dls, metrics=[accuracy, Precision(), Recall()])   \n",
    "\n",
    "                lr_min, lr_steep, lr_valley, lr_slide = model.lr_find(suggest_funcs=(minimum, steep, valley, slide))                \n",
    "                with model.no_bar(), model.no_mbar(), model.no_logging():\n",
    "                    model.fit_one_cycle(2, lr_valley)\n",
    "\n",
    "                dl = model.dls.test_dl(te_df, process=False, bs=4096)\n",
    "                _, acc, pre, rec = model.validate(dl=dl)\n",
    "\n",
    "                accuracies.append(acc)\n",
    "                precisions.append(pre)\n",
    "                recalls.append(rec)\n",
    "            prepared_result_row = [\n",
    "                            dataset_train,\n",
    "                            dataset_test,\n",
    "                            round(np.mean(accuracies), 3),\n",
    "                            round(np.std(accuracies), 3),\n",
    "                            round(np.mean(precisions), 3),\n",
    "                            round(np.std(precisions), 3),\n",
    "                            round(np.mean(recalls), 3),\n",
    "                            round(np.std(recalls), 3),            \n",
    "            ]\n",
    "            print(prepared_result_row)\n",
    "            prepared_result_rows.append(prepared_result_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5d4175",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(figsize=(15, 15))\n",
    "\n",
    "stacked_array_norm = np.stack([row[2] for row in prepared_result_rows])\n",
    "stacked_array_norm=stacked_array_norm.reshape(4,-1)\n",
    "# create heatmap with annotations\n",
    "sns.heatmap(stacked_array_norm, annot=True, fmt='.4f', cmap='YlGnBu', cbar=True, square=True, linewidths=.5, ax=ax)\n",
    "\n",
    "# set title and labels\n",
    "ax.set_title(\"inter-dataset generalization DoS accuracy\")\n",
    "ax.set_xlabel('test')\n",
    "ax.set_ylabel('train')\n",
    "\n",
    "# set x and y ticklabels to categorical labels\n",
    "ax.set_xticklabels(dataset_names)\n",
    "ax.set_yticklabels(dataset_names, rotation=0, ha='right')\n",
    "plt.subplots_adjust(left=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a5d520",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 15))\n",
    "\n",
    "stacked_array_norm = np.stack([row[4] for row in prepared_result_rows])\n",
    "stacked_array_norm=stacked_array_norm.reshape(4,-1)\n",
    "# create heatmap with annotations\n",
    "sns.heatmap(stacked_array_norm, annot=True, fmt='.4f', cmap='YlGnBu', cbar=True, square=True, linewidths=.5, ax=ax)\n",
    "\n",
    "# set title and labels\n",
    "ax.set_title(\"inter-dataset generalization DoS precision\")\n",
    "ax.set_xlabel('test')\n",
    "ax.set_ylabel('train')\n",
    "\n",
    "# set x and y ticklabels to categorical labels\n",
    "ax.set_xticklabels(dataset_names)\n",
    "ax.set_yticklabels(dataset_names, rotation=0, ha='right')\n",
    "plt.subplots_adjust(left=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf56951",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 15))\n",
    "\n",
    "stacked_array_norm = np.stack([row[6] for row in prepared_result_rows])\n",
    "stacked_array_norm=stacked_array_norm.reshape(4,-1)\n",
    "# create heatmap with annotations\n",
    "sns.heatmap(stacked_array_norm, annot=True, fmt='.4f', cmap='YlGnBu', cbar=True, square=True, linewidths=.5, ax=ax)\n",
    "\n",
    "# set title and labels\n",
    "ax.set_title(\"inter-dataset generalization DoS recall\")\n",
    "ax.set_xlabel('test')\n",
    "ax.set_ylabel('train')\n",
    "\n",
    "# set x and y ticklabels to categorical labels\n",
    "ax.set_xticklabels(dataset_names)\n",
    "ax.set_yticklabels(dataset_names, rotation=0, ha='right')\n",
    "plt.subplots_adjust(left=0.2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 101.371676,
   "end_time": "2023-03-29T11:32:35.099136",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-03-29T11:30:53.727460",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
